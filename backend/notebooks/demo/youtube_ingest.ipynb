{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b5a8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "\n",
    "import app.env\n",
    "\n",
    "YOUTUBE_API_KEY = app.env.YOUTUBE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67fd7b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Generator, Optional, Dict, Any\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class VideoInfo:\n",
    "    video_id: str\n",
    "    title: str\n",
    "    channel: str\n",
    "    published_at: datetime\n",
    "    description: str\n",
    "    channel_id: str\n",
    "    view_count: Optional[int] = None\n",
    "    duration: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class CommentInfo:\n",
    "    comment_id: str\n",
    "    video_id: str\n",
    "    text: str\n",
    "    author: str\n",
    "    like_count: int\n",
    "    published_at: datetime\n",
    "    updated_at: datetime\n",
    "    reply_count: int\n",
    "    channel_id: str\n",
    "    author_channel_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import logging\n",
    "\n",
    "from app.schemas import YoutubeVideoInfoSchema\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def search_youtube_videos(\n",
    "    query: str,\n",
    "    max_results: int = 50\n",
    ") -> Generator[YoutubeVideoInfoSchema, None, None]:\n",
    "    \"\"\"\n",
    "    Search for YouTube videos by keyword using a generator.\n",
    "\n",
    "    Yields:\n",
    "        VideoInfo objects one at a time\n",
    "    \"\"\"\n",
    "    if not YOUTUBE_API_KEY:\n",
    "        raise ValueError(\"YouTube API key not configured\")\n",
    "\n",
    "    try:\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "        # Handle pagination\n",
    "        next_page_token = None\n",
    "        videos_yielded = 0\n",
    "\n",
    "        while videos_yielded < max_results:\n",
    "            if max_results > 50:\n",
    "                logger.warning(\"YouTube API limits batch size to 50 results.\")\n",
    "            batch_size = min(50, max_results - videos_yielded)  # YouTube API max is 50\n",
    "\n",
    "            delta_day = datetime.now() - timedelta(days=2)\n",
    "\n",
    "            request = youtube.search().list(\n",
    "                q=query,\n",
    "                part=\"id,snippet\",\n",
    "                maxResults=batch_size,\n",
    "                type=\"video\",\n",
    "                pageToken=next_page_token,\n",
    "                order=\"viewCount\",\n",
    "                publishedAfter=delta_day.isoformat() + 'Z',\n",
    "                relevanceLanguage=\"en\",\n",
    "                regionCode=\"US\",\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Process each video in the response\n",
    "            for item in response.get(\"items\", []):\n",
    "                if videos_yielded >= max_results:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    video_info = _parse_video_item(item)\n",
    "                    yield video_info\n",
    "                    videos_yielded += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing video item: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Check if there are more pages\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token or videos_yielded >= max_results:\n",
    "                break\n",
    "\n",
    "    except HttpError as e:\n",
    "        print(f\"YouTube API error: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "def _parse_video_item(item: Dict[str, Any]) -> VideoInfo:\n",
    "    \"\"\"Parse a YouTube API video item into a VideoInfo object.\"\"\"\n",
    "    snippet = item[\"snippet\"]\n",
    "\n",
    "    return VideoInfo(\n",
    "        video_id=item[\"id\"][\"videoId\"],\n",
    "        title=snippet[\"title\"],\n",
    "        channel=snippet[\"channelTitle\"],\n",
    "        published_at=datetime.fromisoformat(\n",
    "            snippet[\"publishedAt\"].replace('Z', '+00:00')\n",
    "        ),\n",
    "        description=snippet[\"description\"],\n",
    "        channel_id=snippet[\"channelId\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d5c34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Generator, TypeVar\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "def compose_filters(*filters: Callable[[Iterable[T]], Iterable[T]]) -> Callable[[Iterable[T]], Iterable[T]]:\n",
    "    \"\"\"\n",
    "    Compose multiple filter functions for any type.\n",
    "    Each filter should accept and return an iterable of T.\n",
    "    \"\"\"\n",
    "    def composed_filter(items: Iterable[T]) -> Iterable[T]:\n",
    "        current = items\n",
    "        for filter_func in filters:\n",
    "            current = filter_func(current)\n",
    "        return current\n",
    "    return composed_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0238a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import Set, List, Dict, Tuple\n",
    "from langdetect import detect, LangDetectException\n",
    "from emoji import emoji_count\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration constants\n",
    "SPAM_PATTERNS = [\n",
    "    r'(?i)\\b(subscribe|like|follow)\\b.*\\b(back|sub4sub|f4f)\\b',\n",
    "    r'(?i)\\b(check.*out|visit).*\\b(channel|profile|link)\\b',\n",
    "    r'(?i)\\b(make.*money|earn.*\\$|click.*here|amazing.*opportunity)\\b',\n",
    "    r'(?i)\\b(bot|automated|script)\\b',\n",
    "    r'^\\s*[!@#$%^&*()_+=\\[\\]{}|;:,.<>?]*\\s*$',  # Only special characters\n",
    "    r'(?i)\\b(first|early|notification.*squad)\\b',\n",
    "    r'https?://\\S+',  # URLs\n",
    "]\n",
    "\n",
    "def create_comment_fingerprint(text: str) -> str:\n",
    "    \"\"\"Create a normalized fingerprint of the comment.\"\"\"\n",
    "    # Remove special chars, convert to lowercase, remove extra spaces\n",
    "    normalized = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "    # Sort words to catch reordered spam\n",
    "    words = sorted(normalized.split())\n",
    "    return ' '.join(words)\n",
    "\n",
    "def check_spam_patterns(text: str) -> float:\n",
    "    \"\"\"Check for known spam patterns.\"\"\"\n",
    "    matches = sum(1 for pattern in SPAM_PATTERNS if re.search(pattern, text))\n",
    "    return min(matches * 0.3, 0.8)  # Cap at 0.8\n",
    "\n",
    "def check_comment_characteristics(comment: CommentInfo) -> float:\n",
    "    \"\"\"Check comment characteristics that indicate bots.\"\"\"\n",
    "    text = comment.text\n",
    "    score = 0.0\n",
    "\n",
    "    # Very short comments\n",
    "    if len(text.strip()) < 10:\n",
    "        score += 0.2\n",
    "\n",
    "    # Too many emojis\n",
    "    if emoji_count(text) > len(text.split()) * 0.5:\n",
    "        score += 0.3\n",
    "\n",
    "    # All caps\n",
    "    if text.isupper() and len(text) > 10:\n",
    "        score += 0.2\n",
    "\n",
    "    # Excessive punctuation\n",
    "    punct_ratio = len(re.findall(r'[!?]{2,}', text)) / max(len(text), 1)\n",
    "    if punct_ratio > 0.1:\n",
    "        score += 0.2\n",
    "\n",
    "    # No actual words (just numbers/symbols)\n",
    "    words = re.findall(r'\\b[a-zA-Z]{2,}\\b', text)\n",
    "    if len(words) < 2 and len(text) > 5:\n",
    "        score += 0.4\n",
    "\n",
    "    return min(score, 0.8)\n",
    "\n",
    "def check_author_patterns(author: str) -> float:\n",
    "    \"\"\"Check author name patterns that indicate bots.\"\"\"\n",
    "    score = 0.0\n",
    "\n",
    "    # Random character patterns\n",
    "    if re.match(r'^[a-zA-Z]+\\d{4,}$', author):  # Name followed by many digits\n",
    "        score += 0.3\n",
    "\n",
    "    # Generic names\n",
    "    generic_patterns = [\n",
    "        r'(?i)^(user|guest|member|visitor)\\d+',\n",
    "        r'(?i)^[a-z]{6,12}\\d{2,4}$',  # Random letters + numbers\n",
    "    ]\n",
    "\n",
    "    for pattern in generic_patterns:\n",
    "        if re.match(pattern, author):\n",
    "            score += 0.2\n",
    "            break\n",
    "\n",
    "    return min(score, 0.5)\n",
    "\n",
    "def is_bot_comment(comment: CommentInfo) -> Tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    Detect if a comment is likely from a bot.\n",
    "    Returns (is_bot, bot_score)\n",
    "    \"\"\"\n",
    "    reasons = []\n",
    "\n",
    "    # Check for spam patterns\n",
    "    spam_score = check_spam_patterns(comment.text)\n",
    "\n",
    "    # Check comment characteristics\n",
    "    char_score = check_comment_characteristics(comment)\n",
    "\n",
    "    # Check author patterns\n",
    "    author_score = check_author_patterns(comment.author)\n",
    "\n",
    "    # Calculate total score\n",
    "    total_score = spam_score + char_score + author_score\n",
    "\n",
    "    # Add reasons for debugging\n",
    "    if spam_score > 0:\n",
    "        reasons.append(f\"spam({spam_score:.2f})\")\n",
    "    if char_score > 0:\n",
    "        reasons.append(f\"chars({char_score:.2f})\")\n",
    "    if author_score > 0:\n",
    "        reasons.append(f\"author({author_score:.2f})\")\n",
    "\n",
    "    # Normalize score (max possible is ~1.5)\n",
    "    final_score = min(total_score / 1.5, 1.0)\n",
    "    is_bot = final_score > 0.5\n",
    "\n",
    "    if is_bot:\n",
    "        print(f\"ğŸ¤– Bot detected ({final_score:.2f}): {reasons} - '{comment.text[:50]}...'\")\n",
    "\n",
    "    return is_bot, final_score\n",
    "\n",
    "def is_english_comment(comment: CommentInfo) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a comment is in English using multiple methods.\n",
    "    \"\"\"\n",
    "    text = comment.text.strip()\n",
    "\n",
    "    # Skip very short comments or emoji-only comments\n",
    "    if len(text) < 3:\n",
    "        return True  # Give benefit of doubt for very short comments\n",
    "\n",
    "    try:\n",
    "        detected_lang = detect(text)\n",
    "        if detected_lang == 'en':\n",
    "            return True\n",
    "        return False\n",
    "    except LangDetectException:\n",
    "        logger.warning(f\"LangDetect failed for comment: '{text[:50]}...'\")\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "def identify_duplicate_indices(comments: List[CommentInfo]) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Identify all duplicate comment indices (including first occurrence).\n",
    "    Returns a set of indices to filter out.\n",
    "    \"\"\"\n",
    "    fingerprint_counts = Counter()\n",
    "    comment_fingerprints = {}\n",
    "\n",
    "    for i, comment in enumerate(comments):\n",
    "        fingerprint = create_comment_fingerprint(comment.text)\n",
    "        fingerprint_counts[fingerprint] += 1\n",
    "        if fingerprint not in comment_fingerprints:\n",
    "            comment_fingerprints[fingerprint] = []\n",
    "        comment_fingerprints[fingerprint].append(i)\n",
    "\n",
    "    # Mark duplicates (including first occurrence)\n",
    "    duplicate_indices = set()\n",
    "    for fingerprint, count in fingerprint_counts.items():\n",
    "        if count > 1:  # Duplicate found\n",
    "            duplicate_indices.update(comment_fingerprints[fingerprint])\n",
    "\n",
    "    return duplicate_indices\n",
    "\n",
    "def filter_bots(comments: Iterable[CommentInfo]) -> Generator[CommentInfo, None, None]:\n",
    "    \"\"\"Filter out only bot comments, keep duplicates and all languages.\"\"\"\n",
    "    for comment in comments:\n",
    "        if not is_bot_comment(comment)[0]:\n",
    "            yield comment\n",
    "\n",
    "def filter_non_english(comments: Iterable[CommentInfo]) -> Generator[CommentInfo, None, None]:\n",
    "    \"\"\"Filter out only non-English comments, keep bots and duplicates.\"\"\"\n",
    "    for comment in comments:\n",
    "        if is_english_comment(comment):\n",
    "            yield comment\n",
    "        else:\n",
    "            print(f\"ğŸŒ Non-English comment filtered: '{comment.text[:50]}...'\")\n",
    "\n",
    "def filter_all_duplicates(comments: Iterable[CommentInfo]) -> Generator[CommentInfo, None, None]:\n",
    "    \"\"\"\n",
    "    Yield only comments that are not duplicated anywhere else (appear exactly once).\n",
    "    \"\"\"\n",
    "    fingerprints = [create_comment_fingerprint(c.text) for c in comments]\n",
    "    counts = Counter(fingerprints)\n",
    "    for comment in comments:\n",
    "        fingerprint = create_comment_fingerprint(comment.text)\n",
    "        if counts[fingerprint] == 1:\n",
    "            yield comment\n",
    "\n",
    "def sanitise_comments(comments: Iterable[CommentInfo]) -> List[CommentInfo]:\n",
    "    \"\"\"\n",
    "    Sanitise comments using the defined pipeline:\n",
    "    - Filter out bot comments\n",
    "    - Filter out non-English comments\n",
    "    - Filter out all duplicates (keep only unique comments)\n",
    "    \"\"\"\n",
    "    comment_filter_pipeline = compose_filters(\n",
    "        filter_bots,\n",
    "        filter_non_english,\n",
    "        # filter_all_duplicates,\n",
    "    )\n",
    "    return list(comment_filter_pipeline(comments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84ac358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_video_comments(video_id: str, max_results: int = 100) -> Generator[CommentInfo, None, None]:\n",
    "    \"\"\"Yield all comments as a generator for batch processing.\"\"\"\n",
    "    if not YOUTUBE_API_KEY:\n",
    "        raise ValueError(\"YouTube API key not configured\")\n",
    "\n",
    "    try:\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "        comments_yielded = 0\n",
    "        next_page_token = None\n",
    "\n",
    "        while comments_yielded < max_results:\n",
    "            batch_size = min(100, max_results - comments_yielded)\n",
    "\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id,\n",
    "                maxResults=batch_size,\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "            for item in response.get(\"items\", []):\n",
    "                if comments_yielded >= max_results:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    comment_info = _parse_comment_item(item, video_id)\n",
    "\n",
    "                    # Skip channel owner comments\n",
    "                    if comment_info.channel_id == comment_info.author_channel_id:\n",
    "                        continue\n",
    "                    yield comment_info\n",
    "                    comments_yielded += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing comment: {e}\")\n",
    "                    continue\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "    except HttpError as e:\n",
    "        if e.resp.status == 403:\n",
    "            print(f\"Comments disabled for video {video_id} or quota exceeded\")\n",
    "        else:\n",
    "            print(f\"YouTube API error: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting comments: {e}\")\n",
    "        return\n",
    "\n",
    "def _parse_comment_item(item: Dict[str, Any], video_id: str) -> CommentInfo:\n",
    "    \"\"\"Parse a YouTube API comment item into a CommentInfo object.\"\"\"\n",
    "    comment_data = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "\n",
    "    return CommentInfo(\n",
    "        comment_id=item[\"snippet\"][\"topLevelComment\"][\"id\"],\n",
    "        video_id=video_id,\n",
    "        text=comment_data[\"textOriginal\"],\n",
    "        author=comment_data[\"authorDisplayName\"],\n",
    "        like_count=comment_data.get(\"likeCount\", 0),\n",
    "        published_at=datetime.fromisoformat(\n",
    "            comment_data[\"publishedAt\"].replace('Z', '+00:00')\n",
    "        ),\n",
    "        updated_at=datetime.fromisoformat(\n",
    "            comment_data[\"updatedAt\"].replace('Z', '+00:00')\n",
    "        ),\n",
    "        reply_count=item[\"snippet\"][\"totalReplyCount\"],\n",
    "        channel_id=comment_data.get(\"channelId\", \"\"),\n",
    "        author_channel_id=comment_data.get(\"authorChannelId\", {}).get(\"value\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2909171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting comments for: Eth moodğŸ•ºğŸ½dance moodğŸ˜‚ğŸ«£\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LangDetect failed for comment: 'ğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ¥°ğŸ¥°ğŸ¥°ğŸ¥°â¤ï¸â¤ï¸â¤ï¸...'\n",
      "LangDetect failed for comment: 'â¤â¤â¤â¤...'\n",
      "LangDetect failed for comment: 'â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸...'\n",
      "LangDetect failed for comment: 'â¤â¤â¤...'\n",
      "LangDetect failed for comment: 'â¤ğŸ‘ŒğŸ‘Œ...'\n",
      "LangDetect failed for comment: 'â¤â¤â¤ğŸ˜‚...'\n",
      "LangDetect failed for comment: 'â¤ï¸â¤â¤â¤...'\n",
      "LangDetect failed for comment: 'â¤â¤â¤â¤â¤...'\n",
      "LangDetect failed for comment: 'ğŸ˜ğŸ˜ğŸ˜...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'ğŸ‘ŒğŸ‘Œâ¤ï¸â¤ï¸...'\n",
      "ğŸŒ Non-English comment filtered: 'à´à´¨àµà´±àµ† à´¸à´¾à´¯àµ  à´¨à´¿à´¨àµà´±àµ† à´¡à´¾àµ»à´¸àµ à´•à´£àµà´Ÿà´¿à´Ÿàµà´Ÿàµ à´à´šàµà´šàµ‚ à´Ÿàµà´Ÿà´¨àµ à´µà´°àµ†...'\n",
      "ğŸŒ Non-English comment filtered: 'Mom smile so cute â¤â¤â¤...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'MomğŸ˜ğŸ‘ŒğŸ»...'\n",
      "ğŸŒ Non-English comment filtered: 'I â¤ u family...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'â¤â¤â¤â¤â¤â¤â¤...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šâ¤â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'Super super super........'\n",
      "ğŸŒ Non-English comment filtered: 'Amma supr...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ¥°â¤ï¸ğŸ’...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'â¤ğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ˜‚â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'Amma poli â¤ğŸ‰...'\n",
      "ğŸŒ Non-English comment filtered: 'ğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ‘ŒğŸ¥°ğŸ¥°ğŸ¥°ğŸ¥°â¤ï¸â¤ï¸â¤ï¸...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'Poli ğŸ‰ğŸ‰...'\n",
      "ğŸŒ Non-English comment filtered: 'à´…à´®àµà´® ğŸ‘ŒğŸ‘ŒğŸ‘Œà´…à´Ÿà´¿à´ªàµŠà´³à´¿ ğŸ‘ğŸ‘...'\n",
      "ğŸŒ Non-English comment filtered: 'à´à´²àµà´²à´¾à´µà´°àµà´‚ à´¸àµ‚à´ªàµà´ªàµ¼ à´®àµ‚à´¡à´¿àµ½ à´†à´£à´²àµà´²àµ‹ â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸...'\n",
      "ğŸŒ Non-English comment filtered: 'â¤â¤â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'Amme de  dance super â¤â¤â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'Super super super ğŸ‰ğŸ‰ğŸ‰...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'â¤ï¸â¤ï¸â¤ï¸ğŸ‘ğŸ‘...'\n",
      "ğŸŒ Non-English comment filtered: 'à´…à´®àµà´® à´ªàµ†à´³à´¿à´šàµà´šàµ...'\n",
      "ğŸŒ Non-English comment filtered: 'â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸â™¥ï¸...'\n",
      "ğŸŒ Non-English comment filtered: 'à´…à´®àµà´® à´ªàµŠà´³à´¿...'\n",
      "ğŸŒ Non-English comment filtered: 'â¤â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'â¤ğŸ‘ŒğŸ‘Œ...'\n",
      "ğŸŒ Non-English comment filtered: 'Ammaa...poli....â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'Amma danceâ¤...'\n",
      "ğŸŒ Non-English comment filtered: 'ğŸ‘Œà´…à´®àµà´® à´¸àµ‚à´ªàµà´ªàµ¼ â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'Ammaâ¤...'\n",
      "ğŸŒ Non-English comment filtered: 'â¤â¤â¤ğŸ˜‚...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'ğŸ˜®ğŸ˜¢ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚...'\n",
      "ğŸŒ Non-English comment filtered: 'â¤ï¸â¤â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'à´…à´®àµà´®àµ‡ ğŸ˜€ğŸ˜€â¤ï¸â¤ï¸ğŸ‘ğŸ»ğŸ‘ğŸ»à´ªàµŠà´³à´¿à´šàµà´šàµ. à´¹ à´¹ à´¹ ğŸ˜€ğŸ˜€...'\n",
      "ğŸŒ Non-English comment filtered: 'à¤®à¥€ à¤®à¤°à¤¾à¤ à¥€....à¤–à¥‚à¤ª à¤›à¤¾à¤¨ à¤­à¤¾à¤Š....à¤®à¥€ à¤¤à¥à¤®à¤šà¥‡ à¤¸à¤°à¥à¤µ Reel à¤ªà¤¾à¤¹à¤¤...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'â¤ï¸â¤ï¸â¤ï¸...'\n",
      "ğŸŒ Non-English comment filtered: 'â¤â¤â¤â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'à´¹à´¾à´¯àµ ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ¤­ğŸ¤­ğŸ˜€ğŸ˜€ğŸ¤­ğŸ¤­ğŸ˜€ğŸ˜€...'\n",
      "ğŸ¤– Bot detected (0.53): ['chars(0.80)'] - 'â¤â¤â¤â¤â¤â¤...'\n",
      "ğŸŒ Non-English comment filtered: 'ğŸ˜ğŸ˜ğŸ˜...'\n",
      "ğŸŒ Non-English comment filtered: 'Sayoonte dancinoppam  Ammede dancum  aa. Chirium  ...'\n",
      "ğŸŒ Non-English comment filtered: 'Amma dance sooper ğŸ˜ğŸ˜ğŸ˜ğŸ˜...'\n",
      "ğŸŒ Non-English comment filtered: 'ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¥°ğŸ¥°ğŸ¥°à´…à´®àµà´® sayuğŸ¥°ğŸ¥°ğŸ¥°...'\n",
      "\n",
      "âœ… Final human comments: 8\n",
      "\n",
      "1. @ashnaminnu3664 (ğŸ‘ 0)\n",
      "   Comment: Ammaâ¤...\n",
      "\n",
      "2. @Nilamsingh-v8z (ğŸ‘ 0)\n",
      "   Comment: â¤ï¸...\n",
      "\n",
      "3. @sajithmc4127 (ğŸ‘ 0)\n",
      "   Comment: â¤ï¸...\n",
      "\n",
      "4. @nandunandu988 (ğŸ‘ 0)\n",
      "   Comment: ğŸ˜‚...\n",
      "\n",
      "5. @preethamurali1997 (ğŸ‘ 4)\n",
      "   Comment: 1 Million loading soon â¤ğŸ‰...\n"
     ]
    }
   ],
   "source": [
    "crypto_videos = list(\n",
    "    video for _, video in zip(range(3), search_youtube_videos(\"ETH|Ethereum\", max_results=10))\n",
    ")\n",
    "\n",
    "if crypto_videos:\n",
    "    test_video = crypto_videos[1]\n",
    "    print(f\"\\nGetting comments for: {test_video.title}\")\n",
    "\n",
    "    # Get all comments\n",
    "    all_comments = get_all_video_comments(test_video.video_id, max_results=50)\n",
    "    # print(f\"Retrieved {len(all_comments)} total comments\")\n",
    "    # print(\"all_comments:\", all_comments)\n",
    "\n",
    "    # Filter using functional approach\n",
    "    filtered_comments = sanitise_comments(all_comments)\n",
    "\n",
    "    print(f\"\\nâœ… Final human comments: {len(filtered_comments)}\")\n",
    "    for i, comment in enumerate(filtered_comments[:5], 1):\n",
    "        print(f\"\\n{i}. {comment.author} (ğŸ‘ {comment.like_count})\")\n",
    "        print(f\"   Comment: {comment.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96a97a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing non-English comment detection:\n",
      "is_english_comment: True\n"
     ]
    }
   ],
   "source": [
    "non_english_comment = CommentInfo(\n",
    "    comment_id=\"12345\",\n",
    "    video_id=test_video.video_id,\n",
    "    text=\"Top news: $XA19P to the moon ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Š\",\n",
    "    author=\"Test User\",\n",
    "    like_count=10,\n",
    "    published_at=datetime.now(),\n",
    "    updated_at=datetime.now(),\n",
    "    reply_count=0,\n",
    "    channel_id=test_video.channel_id,\n",
    "    author_channel_id=\"UC1234567890abcdefg\"\n",
    ")\n",
    "print(\"\\nTesting non-English comment detection:\")\n",
    "print(f\"is_english_comment: {is_english_comment(non_english_comment)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
